# Learning Retrieval - 质量评估示例

这个示例展示了如何使用 LangChain-Go 的学习型检索系统中的质量评估功能。

## 功能演示

1. **评估单个查询** - 计算单个查询的各项指标
2. **评估检索策略** - 分析策略的整体表现
3. **对比两个策略** - 统计对比不同策略的效果
4. **相关性模型** - 基于用户行为判断文档相关性

## 运行示例

```bash
cd examples/learning_evaluation_demo
go run main.go
```

## 输出示例

```
=== LangChain-Go Learning Retrieval - 质量评估示例 ===

1. 评估单个查询
查询: 什么是深度学习？
策略: hybrid
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
📊 相关性指标:
  Precision:  0.333
  Recall:     1.000
  F1 Score:   0.500
  NDCG:       1.000
  MRR:        1.000

😊 用户满意度:
  评分:       5.0/5.0
  点击率:     66.7%
  阅读率:     33.3%

⭐ 综合得分:
  Overall:    0.808
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

2. 评估检索策略
创建测试数据...

策略 ID: hybrid
总查询数: 11

平均指标:
...

3. 对比两个检索策略
创建策略 B 的测试数据...

策略对比结果:
策略 A: hybrid (0.35 分)
策略 B: vector (0.09 分)

🏆 获胜者: hybrid
📈 提升: 283.52%
✅ 置信度: 85.30%
📊 显著性 (p-value): 0.010
✨ 结果具有统计显著性 (p < 0.05)

4. 相关性模型演示
基于用户行为的相关性评分:
  ✅ doc-A: 0.300 (click)
  ✅ doc-B: 0.800 (click) + read(30s)
  ✅ doc-C: 1.000 (read(120s)) + copy

=== 示例完成 ===
```

## 评估指标说明

### 相关性指标

- **Precision (精确率)**: 检索结果中相关文档的比例
- **Recall (召回率)**: 相关文档中被检索到的比例  
- **F1 Score**: Precision 和 Recall 的调和平均
- **NDCG**: 归一化折损累计增益，考虑文档排序位置
- **MRR**: 平均倒数排名，首个相关文档的位置

### 用户满意度指标

- **评分**: 用户给出的 1-5 星评分
- **点击率 (CTR)**: 被点击的文档占总文档的比例
- **阅读率**: 被阅读的文档占总文档的比例

### 综合得分

综合得分是所有指标的加权平均，范围 0-1：
- NDCG: 25%
- MRR: 15%
- F1: 15%
- 评分: 20%
- CTR: 15%
- 阅读率: 10%

## 相关性模型

系统提供两种相关性模型：

### DefaultRelevanceModel

基于多种用户行为的综合判断：
- 点击: +0.3
- 阅读: +0.5 (长时间阅读 +0.2)
- 复制: +0.4
- 下载: +0.6
- 忽略: -0.3
- 跳过: -0.2

### ImplicitRelevanceModel

可配置权重的隐式反馈模型：
- 点击权重
- 阅读权重
- 时长权重

## 策略对比

对比结果包含：
- **Winner**: 表现更好的策略
- **Improvement**: 提升百分比
- **Confidence**: 置信度 (0-1)
- **SignificantAt**: p-value，< 0.05 表示统计显著

## 应用场景

1. **质量监控**: 持续监控检索质量
2. **A/B 测试**: 科学对比不同策略
3. **参数调优**: 基于评估结果优化参数
4. **问题诊断**: 识别低质量查询并改进

## 下一步

- 查看 **Phase 3: 参数优化** 示例
- 查看 **Phase 4: A/B 测试** 示例
- 探索自定义相关性模型
